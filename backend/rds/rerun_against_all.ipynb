{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('database_config.json') as json_file:\n",
    "    db_config = json.load(json_file)\n",
    "conn = psycopg2.connect(host=db_config['host'], user=db_config['user'], password=db_config['password'], database=db_config['database'], port=db_config['port'])\n",
    "sql = 'SELECT * FROM \"deepcite_call\" ORDER by \"created_at\" DESC;'\n",
    "mega_df = pd.read_sql_query(sql, conn)\n",
    "mega_df = pd.concat([mega_df.drop(['response'], axis=1), mega_df['response'].apply(pd.Series)], axis=1).set_index('id')\n",
    "# mega_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_df.tail(20)\n",
    "3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab all unique submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grab_submission(row):\n",
    "    try:\n",
    "        input = row.results[0]\n",
    "    except:\n",
    "        return  pd.Series([row.results['link'], row.results['source']])\n",
    "    return  pd.Series([input['link'], input['source']])\n",
    "\n",
    "nan_value = float(\"NaN\")\n",
    "submissions = mega_df.dropna(subset=['results']).apply(grab_submission, axis=1)\n",
    "submissions = submissions.drop_duplicates().replace('', nan_value).dropna()\n",
    "submissions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rerun submissions against local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "sys.path.insert(1, '../lambda')\n",
    "from lambda_config import config\n",
    "\n",
    "def current_result(row):\n",
    "    response = requests.post(url=config['ec2']['url'], json={\"claim\": row[1], \"link\": row[0]})\n",
    "    print(response)\n",
    "    return json.loads(response.text)\n",
    "\n",
    "results = submissions.apply(current_result, axis=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun_df = pd.concat([results.drop(['new_response'], axis=1), filtered['new_response'].apply(pd.Series)], axis=1)\n",
    "rerun_df = pd.json_normalize(results)\n",
    "# rerun_df.to_csv('rerun_smaller_model_results.csv', index=False)\n",
    "rerun_df = pd.read_csv('rerun_results.csv')\n",
    "rerun_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_small_df = pd.read_csv('rerun_smaller_model_results.csv')\n",
    "rerun_small_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See differences in results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def print_results(results):\n",
    "    for res in results:\n",
    "        print(res[0][:150])\n",
    "        print(res[1])\n",
    "        print()\n",
    "\n",
    "def matched_results(row1, row2):\n",
    "    res1 = sorted(ast.literal_eval(row1.results), key=lambda k: k['score'], reverse=True)\n",
    "    res2 = sorted(ast.literal_eval(row2.results), key=lambda k: k['score'], reverse=True)\n",
    "\n",
    "    if len(res1) + len(res2) <= 2:\n",
    "        return 'not long enough'\n",
    "\n",
    "    res1 = [(res['link'], res['source']) for res in res1]\n",
    "    res2 = [(res['link'], res['source']) for res in res2]\n",
    "\n",
    "    if res1[:4] != res2[:4]:\n",
    "        print(ast.literal_eval(row1.results)[0]['source'], '\\n')\n",
    "        print_results(res1[1:4])\n",
    "        print('======================')\n",
    "        print_results(res2[1:4])\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        return 'no match in first 3'\n",
    "\n",
    "    if len(res1) != len(res2):\n",
    "        print(len(res1) - len(res2))\n",
    "        return 'length'\n",
    "    \n",
    "    if res1 != res2:\n",
    "        return 'no match'\n",
    "\n",
    "    if row1.error != row2.error:\n",
    "        print(row1.error)\n",
    "        print(row2.error)\n",
    "        print()\n",
    "        return 'error message'\n",
    "\n",
    "    return 'match'\n",
    "\n",
    "matches = []\n",
    "for index, row1 in rerun_df.iterrows():\n",
    "    row2 = rerun_small_df.iloc[index]\n",
    "    matches.append(matched_results(row1, row2))\n",
    "    # if len(matches) > 5:\n",
    "    #     break\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37264bitvenvvenvf5be1d42fc9543cc9620c7a7864bf932",
   "display_name": "Python 3.7.2 64-bit ('v-env': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}