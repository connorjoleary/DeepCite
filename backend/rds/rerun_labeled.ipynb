{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import secretmanager\n",
    "\n",
    "from faunadb import query as q\n",
    "from faunadb.objects import Ref\n",
    "from faunadb.client import FaunaClient\n",
    "\n",
    "def get_client():\n",
    "    print('grabbing secret')\n",
    "\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "    secret_name = \"fauna_deepcite_db\"\n",
    "    project_id = \"deepcite-306405\"\n",
    "\n",
    "    request = {\"name\": f\"projects/{project_id}/secrets/{secret_name}/versions/latest\"}\n",
    "    response = client.access_secret_version(request)\n",
    "    secret_string = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "    return FaunaClient(secret=secret_string, domain='db.us.fauna.com')\n",
    "\n",
    "client = get_client()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "import json\n",
    "\n",
    "def fauna_to_df(collection_name, size = 100000):\n",
    "  fauna_call = client.query(\n",
    "    q.map_(\n",
    "      lambda x: q.get(x),\n",
    "      q.paginate(q.documents(q.collection(collection_name)), size=size)\n",
    "    )\n",
    "  )\n",
    "  data = [doc['data'] for doc in fauna_call['data']]\n",
    "  df = pd.DataFrame.from_records(data)\n",
    "  \n",
    "  times = [datetime.fromtimestamp(doc['ts']/1000000.0) for doc in fauna_call['data']] ## need to figure out timezones\n",
    "  df['created_at'] = times\n",
    "\n",
    "  return df.sort_values(by=['created_at'], ascending=False).reset_index(drop=True)\n",
    "call_df_fauna = fauna_to_df('deepcite_call')\n",
    "call_df_fauna = pd.concat([call_df_fauna.drop(['response'], axis=1), call_df_fauna['response'].apply(pd.Series)], axis=1).set_index('id')\n",
    "call_df_fauna.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "import json\n",
    "\n",
    "with open('database_config.json') as json_file:\n",
    "    db_config = json.load(json_file)['gcp']\n",
    "conn = psycopg2.connect(host=db_config['host'], user=db_config['user'], password=db_config['password'], database=db_config['database'], port=db_config['port'])\n",
    "sql = 'SELECT * FROM \"deepcite_call\" ORDER by \"created_at\" DESC;'\n",
    "call_df = pd.read_sql_query(sql, conn)\n",
    "call_df = pd.concat([call_df.drop(['response'], axis=1), call_df['response'].apply(pd.Series)], axis=1).set_index('id')\n",
    "call_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df_fauna = fauna_to_df('deepcite_source')\n",
    "labels_df_fauna.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'SELECT * FROM \"source_label\" ORDER by \"created_at\" DESC;'\n",
    "labels_df = pd.read_sql_query(sql, conn)\n",
    "labels_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_df_fauna = fauna_to_df('deepcite_retrieval')\n",
    "retrieval_df_fauna.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'SELECT * FROM \"deepcite_retrieval\" ORDER by \"created_at\" DESC;'\n",
    "retrieval_df = pd.read_sql_query(sql, conn)\n",
    "retrieval_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab all unique submissions with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right now I do not know how reliable everyone's labels are, so I'll allowlist some ID's\n",
    "allowed_ids = ['2865b5b498575e748eb26c298eae56688afc9e4045896c8da76ce1931fe0']\n",
    "allowed_labels_df = labels_df[labels_df.user_id.isin(allowed_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_most_recent_redacted(group):\n",
    "    # print(group)\n",
    "\n",
    "    sorted_labels = group.sort_values(['created_at'], ascending=False)\n",
    "    if sorted_labels.iloc[0]['redact']: # i.e. if the most recent label was not redacted\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "grouped = allowed_labels_df.groupby(['base_id', 'source_id'])\n",
    "\n",
    "filtered_labels_df = grouped.filter(check_most_recent_redacted)\n",
    "\n",
    "filtered_size_delta = len(allowed_labels_df.groupby(['base_id', 'source_id'])) - len(filtered_labels_df.groupby(['base_id', 'source_id']))\n",
    "print(f'Filtering reduced the size of the dataframe by: {filtered_size_delta} labels')\n",
    "grouped_labels_df = filtered_labels_df.groupby('base_id')['source_id'].apply(set)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_call = pd.merge(call_df, grouped_labels_df, left_index=True, right_on='base_id', how='inner')\n",
    "print(len(labeled_call))\n",
    "labeled_call.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grab_submission(row):\n",
    "    try:\n",
    "        input = row.results[0]\n",
    "        labels = row.source_id\n",
    "        sources = [[(result['source'], result['link']) for result in row.results if result['citeID'] == label] for label in labels]\n",
    "    except:\n",
    "        return  pd.Series([row.results['link'], row.results['source']])\n",
    "    return  pd.Series([input['link'], input['source'], tuple(sources)])\n",
    "\n",
    "nan_value = float(\"NaN\")\n",
    "submissions = labeled_call.dropna(subset=['results']).apply(grab_submission, axis=1)\n",
    "submissions.columns = ['link', 'claim', 'sources']\n",
    "unique_submissions = submissions.drop_duplicates(subset=['link', 'claim']).replace('', nan_value).dropna()\n",
    "print(len(submissions)-len(unique_submissions))\n",
    "\n",
    "unique_submissions.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rerun submissions against local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "from datetime import date\n",
    "import time\n",
    "\n",
    "def current_result(row):\n",
    "    time.sleep(3)\n",
    "    json.loads('{\"error\": \"timeout\"}')\n",
    "    print(row[0])\n",
    "    try:\n",
    "        response = requests.post(url='http://127.0.0.1:8000/api/v1/deep_cite', json={\"claim\": row[1], \"link\": row[0]})\n",
    "    except:\n",
    "        response = '{\"error\": \"timeout\"}'\n",
    "        print(response)\n",
    "        return json.loads(response)\n",
    "    return response.text\n",
    "\n",
    "results = unique_submissions.apply(current_result, axis=1)\n",
    "unique_submissions['new_results'] = results\n",
    "unique_submissions.to_csv(f'{date.today().strftime(\"%Y_%m_%d\")}_base_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.json_normalize(unique_submissions['new_results'].dropna().apply(lambda x:eval(x)))\n",
    "unique_submissions['new_results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or grab most recent run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "list_of_files = glob.glob('./*.csv') # * means all if need specific format then *.csv\n",
    "latest_file = max(list_of_files, key=os.path.getctime)\n",
    "\n",
    "rerun_df = pd.read_csv(latest_file)\n",
    "rerun_df = pd.concat([rerun_df.drop(['new_results'], axis=1), pd.json_normalize(rerun_df['new_results'].apply(lambda x:eval(x))).add_suffix('_new')], axis=1)\n",
    "rerun_df.set_index('base_id', inplace=True)\n",
    "rerun_df.head(5) \n",
    "\n",
    "# unique_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add row for old results\n",
    "compare_df = rerun_df.join(call_df[['results','error']].add_suffix('_old'))\n",
    "compare_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See differences in results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_labels(row):\n",
    "    '''\n",
    "    Sees how the ranking of each labeled element has changed and sums them.\n",
    "    Higher is a better ranking for the new labels.\n",
    "    '''\n",
    "    if row['error_new'] == 'timeout':\n",
    "        print('timed out for new result')\n",
    "        return float('Nan')\n",
    "\n",
    "    sorted_old = sorted(row['results_old'], key=lambda k: k['score'], reverse=True)\n",
    "    sorted_new = sorted(row['results_new'], key=lambda k: k['score'], reverse=True)\n",
    "\n",
    "    claim_link_old = [(node['source'], node['link']) for node in sorted_old]\n",
    "    claim_link_new = [(node['source'], node['link']) for node in sorted_new]\n",
    "\n",
    "    total_ranking_improvement = 0\n",
    "    for label in eval(row['sources']):\n",
    "        try:\n",
    "            ranking_delta = claim_link_old.index(label[0]) - claim_link_new.index(label[0])\n",
    "        except ValueError:\n",
    "            print('One of the elements wasn\\'t found')\n",
    "            ranking_delta = -0.01\n",
    "        total_ranking_improvement += ranking_delta\n",
    "\n",
    "    return total_ranking_improvement\n",
    "\n",
    "scores = compare_df.apply(compare_labels, axis=1)\n",
    "scores[scores != 0]\n",
    "# compare_df.iloc[0].results_old[0]\n",
    "# compare_df.iloc[0].sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df.loc['e54912df-22e4-42e6-8ad4-75b6c487881c']['results_new']\n",
    "eval(compare_df.loc['e54912df-22e4-42e6-8ad4-75b6c487881c']['sources'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import math\n",
    "\n",
    "def print_results(results):\n",
    "    for res in results:\n",
    "        print(res[0][:150])\n",
    "        print(res[1])\n",
    "        print()\n",
    "\n",
    "def matched_results(results_old, error_old, results_new, error_new):\n",
    "    if error_new == 'timeout':\n",
    "        return 'timed out for new result'\n",
    "\n",
    "    res1 = sorted(results_old, key=lambda k: k['score'], reverse=True)\n",
    "    res2 = sorted(results_new, key=lambda k: k['score'], reverse=True)\n",
    "\n",
    "    if len(res1) + len(res2) <= 2:\n",
    "        return 'not long enough'\n",
    "\n",
    "    res1 = [(res['link'], res['source']) for res in res1]\n",
    "    res2 = [(res['link'], res['source']) for res in res2]\n",
    "\n",
    "    if res1[:4] != res2[:4]:\n",
    "        print(results_old[0]['source'], '\\n')\n",
    "        print_results(res1[1:4])\n",
    "        print('======================')\n",
    "        print_results(res2[1:4])\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        return 'no match in first 3'\n",
    "\n",
    "    if len(res1) != len(res2):\n",
    "        print(len(res1) - len(res2))\n",
    "        return 'length'\n",
    "    \n",
    "    if res1 != res2:\n",
    "        return 'no match'\n",
    "\n",
    "    if error_old != error_new:\n",
    "        print(error_old)\n",
    "        print(error_new)\n",
    "        print()\n",
    "        return 'error message'\n",
    "\n",
    "    return 'match'\n",
    "\n",
    "def compare(row):\n",
    "    return matched_results(row['results_old'], row['error_old'], row['results_new'], row['error_new'])\n",
    "\n",
    "compare_df.apply(compare, axis=1).value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d794fdd8b1589b15a1d4ddc86948b0006b97f30cf6c4e08e223d9b7ec035ef8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('v-env-test': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
